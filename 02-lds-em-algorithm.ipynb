{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda67945",
   "metadata": {},
   "source": [
    "# Linear Dynamical Systems (LDS) in Python\n",
    "\n",
    "Historically Linear dynamical systems (LDS) was developed independently with hidden Markov models (HMM). However, as you will find in this example, this model has the deep relationship with HMM.<br>\n",
    "In this example, I also apply the same approach in hidden Markov models (HMM) example, **EM algorithm**, for Linear dynamical systems (LDS). (See [here](./01-hmm-em-algorithm.ipynb) for HMM.)\n",
    "\n",
    "Unlike HMM, the probability for the latent (hidden) multinomial variables $ \\{\\mathbf{z}_{n}\\} $ is not discrete (i.e, it's continuous). And it then generates the corresponding observation $ \\{\\mathbf{x}_{n}\\} $. (See below.)\n",
    "Same as HMM, the observers can see only $ \\{\\mathbf{x}_{n}\\} $, and the model will be estimated with obervation, $ \\{\\mathbf{x}_{n}\\} $.\n",
    "\n",
    "![Linear Dynamical Systems](images/lds.png?raw=true)\n",
    "\n",
    "In this model, $ p(\\mathbf{z}_{n}|\\mathbf{z}_{n-1}) $ is called a **transition probability**, and $ p(\\mathbf{x}_{n}|\\mathbf{z}_n) $ is a **emission probability**.\n",
    "\n",
    "> In this notebook, I denote a scalar variable by normal letter (such as, $ x $), and denote a vector (incl. a matrix) by bold letter (such as, $ \\mathbf{x} $).\n",
    "\n",
    "*back to [Readme](https://github.com/tsmatz/hmm-lds-em-algorithm/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561acd9",
   "metadata": {},
   "source": [
    "##  Sampling in Linear Dynamical Systems (Generate sample data)\n",
    "\n",
    "First of all, we'll generate sample data (observations) by using the distribution of Linear Dynamical Systems (LDS).",
    "\n",
    "Here I assume that both $ p(\\mathbf{z}_{n}|\\mathbf{z}_{n-1}) $ (transition probability) and $ p(\\mathbf{x}_{n}|\\mathbf{z}_n) $ (emission probability) are a Gaussian distribution as follows. (Note that **I have omitted bias terms** in this example, in order to simplify examples.)\n",
    "\n",
    "$$ p(\\mathbf{z}_{n}|\\mathbf{z}_{n-1}) = \\mathcal{N}(\\mathbf{z}_{n}|\\mathbf{A}\\mathbf{z}_{n-1}, \\mathbf{\\Gamma}) $$\n",
    "\n",
    "$$ p(\\mathbf{x}_{n}|\\mathbf{z}_{n}) = \\mathcal{N}(\\mathbf{x}_{n}|\\mathbf{C}\\mathbf{z}_{n}, \\mathbf{\\Sigma}) $$\n",
    "\n",
    "In this example, I assume that $ \\{ \\mathbf{z}_{n} \\} $ is 3 dimensional space, and $ \\{ \\mathbf{x}_{n} \\} $ is 2 dimensional space.\n",
    "\n",
    "For $ \\{ \\mathbf{x}_{n} \\} $ (observation) sampling, first I'll create the latent (hidden) distribution $ \\{\\mathbf{z}_n\\} $ with the following parameters.<br>\n",
    "$ \\mathbf{A} $ is a rotation matrix in any axis (x, y, and z).\n",
    "\n",
    "$$ \\mathbf{A} = \\begin{bmatrix} 0.750 & 0.433 & -0.500 \\\\ -0.217 & 0.875 & 0.433 \\\\ 0.625 & -0.217 & 0.750 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\Gamma} = \\begin{bmatrix} 1.5 & 0.1 & 0.0 \\\\ 0.1 & 2.0 & 0.3 \\\\ 0.0 & 0.3 & 1.0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c961a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 23.        ,  24.        ,  25.        ],\n",
       "       [ 15.71236153,  27.90702904,  28.17441948],\n",
       "       [  9.28832674,  32.27866956,  25.05656425],\n",
       "       ...,\n",
       "       [-32.29319196,   7.81322471,  76.74943814],\n",
       "       [-59.97730124,  48.80904779,  35.1615163 ],\n",
       "       [-42.51320807,  70.05011635, -21.98105043]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "np.random.seed(1000)  # To evaluate results\n",
    "\n",
    "N = 2000\n",
    "\n",
    "# rotate pi / 6 radian in any axis\n",
    "A = np.matmul(\n",
    "    np.matmul(\n",
    "        np.array([\n",
    "            [1.0,0.0,0.0],\n",
    "            [0.0,math.cos(math.pi/6),math.sin(math.pi/6)],\n",
    "            [0.0,-1.0*math.sin(math.pi/6),math.cos(math.pi/6)]\n",
    "        ]),\n",
    "        np.array([\n",
    "            [math.cos(math.pi/6),0.0,-1.0*math.sin(math.pi/6)],\n",
    "            [0.0,1.0,0.0],\n",
    "            [math.sin(math.pi/6),0.0,math.cos(math.pi/6)]\n",
    "        ])),\n",
    "    np.array([\n",
    "        [math.cos(math.pi/6),math.sin(math.pi/6),0.0],\n",
    "        [-1.0*math.sin(math.pi/6),math.cos(math.pi/6),0.0],\n",
    "        [0.0,0.0,1.0]\n",
    "    ])\n",
    ")\n",
    "\n",
    "Gamma = np.array([\n",
    "    [1.5, 0.1, 0.0],\n",
    "    [0.1, 2.0, 0.3],\n",
    "    [0.0, 0.3, 1.0]\n",
    "])\n",
    "\n",
    "Z = np.array([[23.0, 24.0, 25.0]])\n",
    "for n in range(N):\n",
    "    z_prev = Z[len(Z) - 1]\n",
    "    mean = np.matmul(A, z_prev)\n",
    "    z_post = np.random.multivariate_normal(\n",
    "        mean=mean,\n",
    "        cov=Gamma,\n",
    "        size=1)\n",
    "    Z = np.vstack((Z, z_post))\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751b087",
   "metadata": {},
   "source": [
    "Next I'll create the corresponding observation $ \\{\\mathbf{x}_n\\} $ (2 dimensional space) with the following parameters in $ p(\\mathbf{x}_{n}|\\mathbf{z}_{n}) = \\mathcal{N}(\\mathbf{x}_{n}|\\mathbf{C}\\mathbf{z}_{n}, \\mathbf{\\Sigma}) $.\n",
    "\n",
    "$$ \\mathbf{C} = \\begin{bmatrix} 1.0 & 1.0 & 0.0 \\\\ 0.0 & 1.0 & 1.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\Sigma} = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.2 & 2.0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d18dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 47.68902127,  48.86610347],\n",
       "       [ 43.94953287,  54.53438865],\n",
       "       [ 41.39095419,  55.59938853],\n",
       "       ...,\n",
       "       [-24.57596382,  84.39783143],\n",
       "       [-12.60208378,  84.9819878 ],\n",
       "       [ 29.52247208,  49.04898829]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([\n",
    "    [1.0, 1.0, 0.0],\n",
    "    [0.0, 1.0, 1.0],\n",
    "])\n",
    "\n",
    "Sigma = np.array([\n",
    "    [1.0,0.2],\n",
    "    [0.2,2.0],\n",
    "])\n",
    "\n",
    "X = np.empty((0,2))\n",
    "for z_n in Z:\n",
    "    mean = np.matmul(C, z_n)\n",
    "    x_n = np.random.multivariate_normal(\n",
    "        mean=mean,\n",
    "        cov=Sigma,\n",
    "        size=1)\n",
    "    X = np.vstack((X, x_n))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb896c56",
   "metadata": {},
   "source": [
    "## EM algorithm in Linear Dynamical Systems (LDS)\n",
    "\n",
    "Now, using the given observation $ \\{ \\mathbf{x}_n \\} $, let's try to get the optimimal parameters in LDS.\n",
    "\n",
    "When I denote unknown parameters by $ \\mathbf{\\theta} $, our goal is to get the optimal parameters $ \\mathbf{\\theta} $ to maximize the following (1).\n",
    "\n",
    "$$ p(\\mathbf{X}|\\mathbf{\\theta}) = \\sum_{\\mathbf{Z}} p(\\mathbf{X},\\mathbf{Z}|\\mathbf{\\theta}) \\;\\;\\;\\;(1) $$\n",
    "\n",
    "where $ \\mathbf{Z} = \\{\\mathbf{z}_n\\} $ and $ \\mathbf{X} = \\{\\mathbf{x}_n\\} $\n",
    "\n",
    "As I have mentioned in [here](./01-hmm-em-algorithm.ipynb), it's difficult to apply [maximum likelihood estimation (MLE)](https://tsmatz.wordpress.com/2017/08/30/glm-regression-logistic-poisson-gaussian-gamma-tutorial-with-r/) for the expression (1), and I will then apply **EM algorithm** (**E**xpectationâ€“**M**aximization algorithm) to solve unknown parameters.\n",
    "\n",
    "In EM algorithm for LDS, we start with initial parameters $ \\mathbf{\\theta}^{old} $, and optimize (find) new $ \\mathbf{\\theta} $ to maximize the following expression (2).<br>\n",
    "By repeating this operation, we can expect to reach to the likelihood parameters $ \\hat{\\mathbf{\\theta}} $.\n",
    "\n",
    "$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{old}) = \\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X}, \\mathbf{\\theta}^{old}) \\ln p(\\mathbf{X}, \\mathbf{Z}|\\mathbf{\\theta}) \\;\\;\\;\\;(2) $$\n",
    "\n",
    "> Note : For the essential idea of EM algorithm, see Chapter 9 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft)\n",
    "\n",
    "In LDS, we use  the following parameters as $ \\mathbf{\\theta} = \\{ \\mathbf{m}_0, \\mathbf{S}_0, \\mathbf{A}, \\mathbf{\\Gamma}, \\mathbf{C}, \\mathbf{\\Sigma} \\} $.\n",
    "\n",
    "- $ \\mathbf{m}_0, \\mathbf{S}_0 $ : Gaussian distribution's parameters (mean, variance) in initial latent node $ p(\\mathbf{z}_0) = \\mathcal{N}(\\mathbf{m}_0, \\mathbf{S}_0) $.\n",
    "- $ \\mathbf{A}, \\mathbf{\\Gamma} $ : parameters in transition probability $ p(\\mathbf{z}_{n}|\\mathbf{z}_{n-1}) = \\mathcal{N}(\\mathbf{z}_{n}|\\mathbf{A}\\mathbf{z}_{n-1}, \\mathbf{\\Gamma}) $.<br>\n",
    "(In this example, I have omitted a bias term for simplicity.)\n",
    "- $ \\mathbf{C}, \\mathbf{\\Sigma} $ : parameters in emission probability $ p(\\mathbf{x}_{n}|\\mathbf{z}_{n}) = \\mathcal{N}(\\mathbf{x}_{n}|\\mathbf{C}\\mathbf{z}_{n}, \\mathbf{\\Sigma}) $.<br>\n",
    "(In this example, I have omitted a bias term for simplicity.)\n",
    "\n",
    "Now I denote the probability $ p(\\mathbf{z}_n|\\mathbf{x}_1, \\ldots, \\mathbf{x}_n, \\mathbf{\\theta}^{old}) $ by $ \\alpha(z_n) = \\mathcal{N}(\\mathbf{z}_{n}|\\mathbf{\\mu}_n, \\mathbf{V}_n) $.<br>\n",
    "It's then known that $ \\mathbf{\\mu}_n  $ and $ \\mathbf{V}_n $ can be recursively given by $ \\mathbf{\\theta}^{old} , \\mathbf{X} $ as follows.\n",
    "\n",
    "$$ \\mathbf{\\mu}_n = \\mathbf{A}^{old} \\mathbf{\\mu}_{n-1} + \\mathbf{K}_{n-1} (\\mathbf{x}_n - \\mathbf{C}^{old} \\mathbf{A}^{old} \\mathbf{\\mu}_{n-1}) $$\n",
    "\n",
    "$$ \\mathbf{V}_n = (\\mathbf{I} - \\mathbf{K}_{n-1} \\mathbf{C}^{old}) \\mathbf{P}_{n-1} $$\n",
    "\n",
    "where\n",
    "\n",
    "$ \\mathbf{P}_{n} = \\mathbf{A}^{old} \\mathbf{V}_{n} (\\mathbf{A}^{old})^{T} + \\mathbf{\\Gamma}^{old} $\n",
    "\n",
    "and\n",
    "\n",
    "$ \\mathbf{K}_n = \\mathbf{P}_{n} (\\mathbf{C}^{old})^T (\\mathbf{C}^{old} \\mathbf{P}_{n} (\\mathbf{C}^{old})^T + \\mathbf{\\Sigma}^{old})^{-1} $\n",
    "\n",
    "> Note : In [HMM](./01-hmm-em-algorithm.ipynb), $ \\alpha(z_n) $ is defined as $ \\alpha(z_n)=p(\\mathbf{x}_1, \\ldots, \\mathbf{x}_n, \\mathbf{z}_n |\\mathbf{\\theta}^{old}) $. As I mentioned in HMM, the series of these variables $ \\alpha(z_n) $ will go to zero exponentially, when $ N $ is large. It will then eventually exceed the dynamic range of precision in computation, and we should introduce a scaling factor in each step $ n $.<br>\n",
    "> In this example, $ \\alpha(z_n) $ is scaled variables $ \\alpha(z_n)=p(\\mathbf{z}_n|\\mathbf{x}_1, \\ldots, \\mathbf{x}_n, \\mathbf{\\theta}^{old}) $ and you don't need to scale. Therefore, when you monitor the value of likelihood functions, you also need to calculate scaling factors $ \\{ c_n \\} $ in each step. (See Chapter 13 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft) for details.)\n",
    "\n",
    "In this recursion, the starting condition $ \\mathbf{\\mu}_0, \\mathbf{V}_0 $ is :\n",
    "\n",
    "$$ \\mathbf{\\mu}_0 = \\mathbf{m}_0^{old} + \\mathbf{K}_0 (\\mathbf{x}_0 - \\mathbf{C}^{old} \\mathbf{m}_0^{old}) $$\n",
    "\n",
    "$$ \\mathbf{V}_0 = (\\mathbf{I} - \\mathbf{S}_0^{old} (\\mathbf{C}^{old})^T (\\mathbf{C}^{old} \\mathbf{S}_0^{old} (\\mathbf{C}^{old})^T + \\mathbf{\\Sigma}^{old})^{-1} \\mathbf{C}^{old}) \\mathbf{S}_0^{old} $$\n",
    "\n",
    "> Note : When we assume $ \\mathbf{P}_{-1} = \\mathbf{S}_0 $, then $ \\mathbf{V}_0 $ is denoted by $ (\\mathbf{I} - \\mathbf{K}_{-1} \\mathbf{C}^{old}) \\mathbf{P}_{-1} $. As you can find, it is consistent with the previous expression.<br>\n",
    "> This $ \\mathbf{K} $ is called a Kalman gain matrix.<br>\n",
    "> Same as $ \\mathbf{V}_0 $ and $ \\mathbf{S}_0 $, $ \\mathbf{\\mu}_0 $ is also related with $ \\mathbf{m}_0 $.\n",
    "\n",
    "Now I also denote the probability $ p(\\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $ by $ \\gamma(z_n) = \\mathcal{N}(\\mathbf{z}_{n}|\\hat{\\mathbf{\\mu}}_n, \\hat{\\mathbf{V}}_n) $.\n",
    "\n",
    "Once you've done previous forward recursion for $ \\{ \\mathbf{\\mu}_n \\} $ and $ \\{ \\mathbf{V}_n \\} $, run the following backward recursion and then get $ \\{ \\hat{\\mathbf{\\mu}}_n \\}  $ and $ \\{ \\hat{\\mathbf{V}}_n \\} $ by using $ \\mathbf{\\theta}^{old} , \\mathbf{X} $, and previous $ \\{ \\mathbf{\\mu}_n \\}, \\{ \\mathbf{V}_n \\} $.\n",
    "\n",
    "$$ \\hat{\\mathbf{\\mu}}_n = \\mathbf{\\mu}_n + \\mathbf{J}_n (\\hat{\\mathbf{\\mu}}_{n+1} - \\mathbf{A}^{old} \\mathbf{\\mu}_n) $$\n",
    "\n",
    "$$ \\hat{\\mathbf{V}}_n = \\mathbf{V}_n + \\mathbf{J}_n (\\hat{\\mathbf{V}}_{n+1} - \\mathbf{P}_n) \\mathbf{J}_n^T $$\n",
    "\n",
    "where $ \\mathbf{J}_n = \\mathbf{V}_n (\\mathbf{A}^{old})^T (\\mathbf{P}_n)^{-1} $.\n",
    "\n",
    "Once you have got these variables, now you can get the following expectation values.\n",
    "\n",
    "$$ \\mathbb{E}[\\mathbf{z}_n] = \\hat{\\mathbf{\\mu}}_n $$\n",
    "\n",
    "$$ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^T] = \\hat{\\mathbf{V}}_n \\mathbf{J}_{n-1}^{T} + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_{n-1}^T $$\n",
    "\n",
    "$$ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] = \\hat{\\mathbf{V}}_n + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_n^T $$\n",
    "\n",
    "> Note : $ \\mathbb{E}[\\mathbf{z}_{n-1} \\mathbf{z}_{n}^T] = \\left( \\hat{\\mathbf{V}}_n \\mathbf{J}_{n-1}^{T} + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_{n-1}^T \\right)^T $\n",
    "\n",
    "With these expectation values, you can get the optimal $ \\mathbf{\\theta} = \\{ \\mathbf{m}_0, \\mathbf{S}_0, \\mathbf{A}, \\mathbf{\\Gamma}, \\mathbf{C}, \\mathbf{\\Sigma} \\} $ to maximize (2) as follows.\n",
    "\n",
    "$$ \\mathbf{m}_0^{new} = \\mathbb{E}[\\mathbf{z}_0] $$\n",
    "\n",
    "$$ \\mathbf{S}_0^{new} = \\mathbb{E}[\\mathbf{z}_0 \\mathbf{z}_0^T] - \\mathbb{E}[\\mathbf{z}_0]\\mathbb{E}[\\mathbf{z}_0^T] $$\n",
    "\n",
    "$$ \\mathbf{A}^{new} = \\left( \\sum_{n=1}^{N-1} \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^T] \\right) \\left( \\sum_{n=0}^{N-2} \\mathbb{E}[\\mathbf{z}_{n} \\mathbf{z}_{n}^T] \\right)^{-1} $$\n",
    "\n",
    "$$ \\mathbf{\\Gamma}^{new} = \\frac{1}{N - 1} \\sum_{n=1}^{N-1} \\left\\{ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] - \\mathbf{A}^{new} \\mathbb{E}[\\mathbf{z}_{n-1} \\mathbf{z}_n^T] - \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^{T}](\\mathbf{A}^{new})^T + \\mathbf{A}^{new} \\mathbb{E}[\\mathbf{z}_{n-1}\\mathbf{z}_{n-1}^T](\\mathbf{A}^{new})^T \\right\\} $$\n",
    "\n",
    "$$ \\mathbf{C}^{new} = \\left( \\sum_{n=0}^{N-1} \\mathbf{x}_n \\mathbb{E}[\\mathbf{z}_n]^T \\right) \\left( \\sum_{n=0}^{N-1} \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] \\right)^{-1} $$\n",
    "\n",
    "$$ \\mathbf{\\Sigma}^{new} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left\\{ \\mathbf{x}_n \\mathbf{x}_n^T - \\mathbf{C}^{new} \\mathbb{E}[\\mathbf{z}_n] \\mathbf{x}_n^T - \\mathbf{x}_n \\mathbb{E}[\\mathbf{z}_n^T](\\mathbf{C}^{new})^T + \\mathbf{C}^{new} \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] (\\mathbf{C}^{new})^T \\right\\} $$\n",
    "\n",
    "You should repeat this process by replacing $ \\mathbf{\\theta}^{old} $ with new $ \\mathbf{\\theta} $, and you will eventually get the optimal results $ \\hat{\\mathbf{\\theta}} $.\n",
    "\n",
    "> Note : For these properties, please refer Chapter 13 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1a21e",
   "metadata": {},
   "source": [
    "## Apply algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60f500",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7be26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3be6ac",
   "metadata": {},
   "source": [
    "## 1. Initialize parameters\n",
    "\n",
    "First, initialize $ \\mathbf{\\theta} = \\{ \\mathbf{m}_0, \\mathbf{S}_0, \\mathbf{A}, \\mathbf{\\Gamma}, \\mathbf{C}, \\mathbf{\\Sigma} \\} $ as follows.<br>\n",
    "In this example, I set the primitive fixed values.\n",
    "\n",
    "- $ \\mathbf{m}_0 = (10.0, 10.0, 10.0) $\n",
    "- $ \\mathbf{S}_0 = \\begin{bmatrix} 1.0 & 0.5 & 0.5 \\\\ 0.5 & 1.0 & 0.5 \\\\ 0.5 & 0.5 & 1.0 \\end{bmatrix} $\n",
    "- $ \\mathbf{A} = \\begin{bmatrix} 1.0 & 1.1 & 1.2 \\\\ 1.3 & 1.4 & 1.5 \\\\ 1.6 & 1.7 & 1.8 \\end{bmatrix} $\n",
    "- $ \\mathbf{\\Gamma} = \\begin{bmatrix} 1.0 & 0.5 & 0.5 \\\\ 0.5 & 1.0 & 0.5 \\\\ 0.5 & 0.5 & 1.0 \\end{bmatrix} $\n",
    "- $ \\mathbf{C} = \\begin{bmatrix} 1.0 & 1.0 & 1.0 \\\\ 1.0 & 1.0 & 1.0 \\end{bmatrix} $\n",
    "- $ \\mathbf{\\Sigma} = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.5 & 1.0 \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b114a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "class theta:\n",
    "    m0 = np.empty((0,3))\n",
    "    S0 = np.empty((0,3,3))\n",
    "    A = np.empty((0,3,3))\n",
    "    Gamma = np.empty((0,3,3))\n",
    "    C = np.empty((0,2,3))\n",
    "    Sigma = np.empty((0,2,2))\n",
    "\n",
    "    def __init__(self, m0, S0, A, Gamma, C, Sigma):\n",
    "        self.m0 = m0\n",
    "        self.S0 = S0\n",
    "        self.A = A\n",
    "        self.Gamma = Gamma\n",
    "        self.C = C\n",
    "        self.Sigma = Sigma\n",
    "\n",
    "theta_old = theta(\n",
    "    m0=np.array([10.0, 10.0, 10.0]),\n",
    "    S0=np.array([\n",
    "        [1.0, 0.5, 0.5],\n",
    "        [0.5, 1.0, 0.5],\n",
    "        [0.5, 0.5, 1.0]\n",
    "    ]),\n",
    "    A=np.array([\n",
    "        [1.0, 1.1, 1.2],\n",
    "        [1.3, 1.4, 1.5],\n",
    "        [1.6, 1.7, 1.8]\n",
    "    ]),\n",
    "    Gamma=np.array([\n",
    "        [1.0, 0.5, 0.5],\n",
    "        [0.5, 1.0, 0.5],\n",
    "        [0.5, 0.5, 1.0]\n",
    "    ]),\n",
    "    C=np.array([\n",
    "        [1.0, 1.0, 1.0],\n",
    "        [1.0, 1.0, 1.0],\n",
    "    ]),\n",
    "    Sigma=np.array([\n",
    "        [1.0, 0.5],\n",
    "        [0.5, 1.0]\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb646728",
   "metadata": {},
   "source": [
    "## 2. Get $ \\{ \\mathbf{\\mu}_n \\} $ and $ \\{ \\mathbf{V}_n \\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619b21d7",
   "metadata": {},
   "source": [
    "Now I set the starting condition, $ \\mathbf{V}_0 $ as follows. :\n",
    "\n",
    "$$ \\mathbf{V}_0 = (\\mathbf{I} - \\mathbf{S}_0^{old} (\\mathbf{C}^{old})^T (\\mathbf{C}^{old} \\mathbf{S}_0^{old} (\\mathbf{C}^{old})^T + \\mathbf{\\Sigma}^{old})^{-1} \\mathbf{C}^{old}) \\mathbf{S}_0^{old} $$\n",
    "\n",
    "where $ \\mathbf{P}_{n} = \\mathbf{A}^{old} \\mathbf{V}_{n} (\\mathbf{A}^{old})^{T} + \\mathbf{\\Gamma}^{old} $ and $ \\mathbf{K}_n = \\mathbf{P}_{n} (\\mathbf{C}^{old})^T (\\mathbf{C}^{old} \\mathbf{P}_{n} (\\mathbf{C}^{old})^T + \\mathbf{\\Sigma}^{old})^{-1} $\n",
    "\n",
    "As I mentioned above, this is equivalent to :\n",
    "\n",
    "$$ \\mathbf{V}_0 = (\\mathbf{I} - \\mathbf{K}_{-1} \\mathbf{C}^{old}) \\mathbf{S}_0^{old} $$\n",
    "\n",
    "where $ \\mathbf{K}_{-1} = \\mathbf{S}_0^{old} (\\mathbf{C}^{old})^T (\\mathbf{C}^{old} \\mathbf{S}_0^{old} (\\mathbf{C}^{old})^T + \\mathbf{\\Sigma}^{old})^{-1} $\n",
    "\n",
    "And we can recursively obtain all $ \\{ \\mathbf{V}_n \\} $ as follows.\n",
    "\n",
    "$$ \\mathbf{V}_n = (\\mathbf{I} - \\mathbf{K}_{n-1} \\mathbf{C}^{old}) \\mathbf{P}_{n-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c6c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def P(V_n):\n",
    "    res = np.matmul(theta_old.A, V_n)\n",
    "    res = np.matmul(res, theta_old.A.transpose())\n",
    "    res = res + theta_old.Gamma\n",
    "    return res\n",
    "\n",
    "def K(P_n):\n",
    "    res = np.matmul(P_n, theta_old.C.transpose())\n",
    "    inv = np.matmul(theta_old.C, P_n)\n",
    "    inv = np.matmul(inv, theta_old.C.transpose())\n",
    "    inv = inv + theta_old.Sigma\n",
    "    inv = np.linalg.inv(inv)\n",
    "    res = np.matmul(res, inv)\n",
    "    return res\n",
    "\n",
    "def get_V():\n",
    "    V = np.empty((0,3,3))\n",
    "    I = np.array([[1.0,0.0,0.0],[0.0,1.0,0.0],[0.0,0.0,1.0]])\n",
    "\n",
    "    # Get initial V_0\n",
    "    K_minus1 = K(theta_old.S0)\n",
    "    V_0 = np.matmul(\n",
    "        np.subtract(I, np.matmul(K_minus1, theta_old.C)),\n",
    "        theta_old.S0)\n",
    "    V = np.vstack((V, [V_0]))\n",
    "\n",
    "    # Get all elements recursively\n",
    "    for n in range(1, N):\n",
    "        P_n_minus1 = P(V[n-1])\n",
    "        V_n = np.matmul(\n",
    "            np.subtract(I, np.matmul(K(P_n_minus1),theta_old.C)),\n",
    "            P_n_minus1)\n",
    "        V = np.vstack((V, [V_n]))\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a4fc8",
   "metadata": {},
   "source": [
    "I also set the starting condition, $ \\mathbf{\\mu}_0 $ as follows. :\n",
    "\n",
    "$$ \\mathbf{\\mu}_0 = \\mathbf{m}_0^{old} + \\mathbf{K}_0 (\\mathbf{x}_0 - \\mathbf{C}^{old} \\mathbf{m}_0^{old}) $$\n",
    "\n",
    "And we can recursively obtain all $ \\{ \\mathbf{\\mu}_n \\} $ as follows.\n",
    "\n",
    "$$ \\mathbf{\\mu}_n = \\mathbf{A}^{old} \\mathbf{\\mu}_{n-1} + \\mathbf{K}_{n-1} (\\mathbf{x}_n - \\mathbf{C}^{old} \\mathbf{A}^{old} \\mathbf{\\mu}_{n-1}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179fc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu(V):\n",
    "    mu = np.empty((0,3))\n",
    "\n",
    "    # Get initial mu_0\n",
    "    P_0 = P(V[0])\n",
    "    K_0 = K(P_0)\n",
    "    theta_old_m0_T = np.array([theta_old.m0]).transpose()\n",
    "    X_0_T = np.array([X[0]]).transpose()\n",
    "    mu_0_T = np.add(\n",
    "        theta_old_m0_T,\n",
    "        np.matmul(\n",
    "            K_0,\n",
    "            np.subtract(\n",
    "                X_0_T,\n",
    "                np.matmul(theta_old.C,theta_old_m0_T)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    mu_0 = np.squeeze(mu_0_T.transpose())\n",
    "    mu = np.vstack((mu, mu_0))\n",
    "\n",
    "    # Get all elements recursively\n",
    "    for n in range(1, N):\n",
    "        P_n_minus1 = P(V[n-1])\n",
    "        K_n_minus1 = K(P_n_minus1)\n",
    "        mu_n_minus1_T = np.array([mu[n-1]]).transpose()\n",
    "        mu_n_former_T = np.matmul(theta_old.A, mu_n_minus1_T)\n",
    "        X_n_T = np.array([X[n]]).transpose()\n",
    "        sub_n_T = np.subtract(\n",
    "            X_n_T,\n",
    "            np.matmul(\n",
    "                np.matmul(theta_old.C,theta_old.A),\n",
    "                mu_n_minus1_T\n",
    "            )\n",
    "        )\n",
    "        mu_n_latter_T = np.matmul(\n",
    "            K_n_minus1,\n",
    "            sub_n_T)\n",
    "        mu_n_T = np.add(mu_n_former_T, mu_n_latter_T)\n",
    "        mu_n = np.squeeze(mu_n_T.transpose())\n",
    "        mu = np.vstack((mu, mu_n))\n",
    "\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486bdfc",
   "metadata": {},
   "source": [
    "## 3. Get $ \\{ \\hat{\\mathbf{\\mu}}_n \\} $ and $ \\{ \\hat{\\mathbf{V}}_n \\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee549e91",
   "metadata": {},
   "source": [
    "Now we obtain $ \\{ \\hat{\\mathbf{\\mu}}_n \\} $ and $ \\{ \\hat{\\mathbf{V}}_n \\} $ by running backward recursion with $ \\{ \\mathbf{\\mu}_n \\} $ and $ \\{ \\mathbf{V}_n \\} $.\n",
    "\n",
    "First we get $ \\{ \\mathbf{V}_n \\} $ as follows.\n",
    "\n",
    "$$ \\hat{\\mathbf{V}}_n = \\mathbf{V}_n + \\mathbf{J}_n (\\hat{\\mathbf{V}}_{n+1} - \\mathbf{P}_n) \\mathbf{J}_n^T $$\n",
    "\n",
    "where $ \\mathbf{J}_n = \\mathbf{V}_n (\\mathbf{A}^{old})^T (\\mathbf{P}_n)^{-1} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4282314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(V_n, P_n):\n",
    "    return np.matmul(\n",
    "        np.matmul(V_n,theta_old.A.transpose()),\n",
    "        np.linalg.inv(P_n)\n",
    "    )\n",
    "\n",
    "def get_V_hat(V):\n",
    "    V_hat_rev = np.empty((0,3,3))\n",
    "    V_hat_rev = np.vstack((V_hat_rev, [V[N-1]]))\n",
    "\n",
    "    for n in range(1, N):\n",
    "        V_n = V[N-n-1]\n",
    "        P_n = P(V_n)\n",
    "        J_n = J(V_n, P_n)\n",
    "        V_hat_n = np.add(\n",
    "            V_n,\n",
    "            np.matmul(\n",
    "                np.matmul(\n",
    "                    J_n,\n",
    "                    np.subtract(V_hat_rev[n-1],P_n)\n",
    "                ),\n",
    "                J_n.transpose()\n",
    "            )\n",
    "        )\n",
    "        V_hat_rev = np.vstack((V_hat_rev, [V_hat_n]))\n",
    "\n",
    "    # Reverse results\n",
    "    V_hat = np.flip(V_hat_rev, axis=0)\n",
    "\n",
    "    return V_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0da770",
   "metadata": {},
   "source": [
    "Next we also get $ \\{ \\hat{\\mathbf{\\mu}}_n \\} $ as follows.\n",
    "\n",
    "$$ \\hat{\\mathbf{\\mu}}_n = \\mathbf{\\mu}_n + \\mathbf{J}_n (\\hat{\\mathbf{\\mu}}_{n+1} - \\mathbf{A}^{old} \\mathbf{\\mu}_n) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec9dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_hat(mu, V):\n",
    "    mu_hat_rev = np.empty((0,3))\n",
    "    mu_hat_rev = np.vstack((mu_hat_rev, mu[N-1]))\n",
    "\n",
    "    for n in range(1, N):\n",
    "        mu_n = mu[N-n-1]\n",
    "        mu_n_T = np.array([mu_n]).transpose()\n",
    "        mu_hat_rev_n_minus1_T = np.array([mu_hat_rev[n-1]]).transpose()\n",
    "        V_n = V[N-n-1]\n",
    "        P_n = P(V_n)\n",
    "        J_n = J(V_n, P_n)\n",
    "        mu_hat_n_T = np.add(\n",
    "            mu_n_T,\n",
    "            np.matmul(\n",
    "                J_n,\n",
    "                np.subtract(\n",
    "                    mu_hat_rev_n_minus1_T,\n",
    "                    np.matmul(theta_old.A,mu_n_T)\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        mu_hat_n = np.squeeze(mu_hat_n_T.transpose())\n",
    "        mu_hat_rev = np.vstack((mu_hat_rev, mu_hat_n))\n",
    "\n",
    "    # Reverse results\n",
    "    mu_hat = np.flip(mu_hat_rev, axis=0)\n",
    "\n",
    "    return mu_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fd509",
   "metadata": {},
   "source": [
    "## 4. Get Expectations\n",
    "\n",
    "Now we get the following array of expectations.\n",
    "\n",
    "1. $ \\mathbb{E}[\\mathbf{z}_n] = \\hat{\\mathbf{\\mu}}_n \\;\\; (n=0, \\ldots, N-1) $\n",
    "2. $ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^T] = \\hat{\\mathbf{V}}_n \\mathbf{J}_{n-1}^{T} + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_{n-1}^T \\;\\; (n=1, \\ldots, N-1) $\n",
    "3. $ \\mathbb{E}[\\mathbf{z}_{n-1} \\mathbf{z}_{n}^T] = \\left( \\hat{\\mathbf{V}}_n \\mathbf{J}_{n-1}^{T} + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_{n-1}^T \\right)^T \\;\\; (n=1, \\ldots, N-1) $\n",
    "4. $ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] = \\hat{\\mathbf{V}}_n + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_n^T \\;\\; (n=0, \\ldots, N-1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4b49e1",
   "metadata": {},
   "source": [
    "### (1) $ \\mathbb{E}[\\mathbf{z}_n] = \\hat{\\mathbf{\\mu}}_n \\;\\; (n=0, \\ldots, N-1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "225e3890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_E1(mu_hat):\n",
    "    return mu_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb1d88",
   "metadata": {},
   "source": [
    "### (2) $ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^T] = \\hat{\\mathbf{V}}_n \\mathbf{J}_{n-1}^{T} + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_{n-1}^T  \\;\\; (n=1, \\ldots, N-1)$\n",
    "\n",
    "### (3) $ \\mathbb{E}[\\mathbf{z}_{n-1} \\mathbf{z}_{n}^T] = \\left( \\hat{\\mathbf{V}}_n \\mathbf{J}_{n-1}^{T} + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_{n-1}^T \\right)^T \\;\\; (n=1, \\ldots, N-1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e82019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_E2_E3(V, mu_hat, V_hat):\n",
    "    E2 = np.empty((0,3,3))\n",
    "    E3 = np.empty((0,3,3))\n",
    "    for n in range(1,N):\n",
    "        P_n_minus1 = P(V[n-1])\n",
    "        J_n_minus1 = J(V[n-1], P_n_minus1)\n",
    "        mu_hat_n_T = np.array([mu_hat[n]]).transpose()\n",
    "        mu_hat_n_minus1 = np.array([mu_hat[n-1]])\n",
    "        E2_n = np.add(\n",
    "            np.matmul(\n",
    "                V_hat[n],\n",
    "                J_n_minus1.transpose()\n",
    "            ),\n",
    "            np.matmul(\n",
    "                mu_hat_n_T,\n",
    "                mu_hat_n_minus1\n",
    "            )\n",
    "        )\n",
    "        E2 = np.vstack((E2, [E2_n]))\n",
    "        E3_n = E2_n.transpose()\n",
    "        E3 = np.vstack((E3, [E3_n]))\n",
    "    return E2, E3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc0c74e",
   "metadata": {},
   "source": [
    "### (4) $ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] = \\hat{\\mathbf{V}}_n + \\hat{\\mathbf{\\mu}}_n \\hat{\\mathbf{\\mu}}_n^T \\;\\; (n=0, \\ldots, N-1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d1777ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_E4(mu_hat, V_hat):\n",
    "    E4 = np.empty((0,3,3))\n",
    "    for n in range(N):\n",
    "        mu_hat_n = np.array([mu_hat[n]])\n",
    "        mu_hat_n_T = mu_hat_n.transpose()\n",
    "        E4_n = np.add(\n",
    "            V_hat[n],\n",
    "            np.matmul(mu_hat_n_T, mu_hat_n)\n",
    "        )\n",
    "        E4 = np.vstack((E4, [E4_n]))\n",
    "    return E4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27e5dc",
   "metadata": {},
   "source": [
    "## 5. Get new (optimal) parameters $ \\mathbf{\\theta} $\n",
    "\n",
    "Finally, get new $ \\mathbf{\\theta} = \\{ \\mathbf{m}_0, \\mathbf{S}_0, \\mathbf{A}, \\mathbf{\\Gamma}, \\mathbf{C}, \\mathbf{\\Sigma} \\} $ using previous E1, E2, E3, and E4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60333624",
   "metadata": {},
   "source": [
    "First, $ \\mathbf{m}_0 $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{m}_0^{new} = \\mathbb{E}[\\mathbf{z}_0] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e03c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m0_new(E1):\n",
    "    return E1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e71ec",
   "metadata": {},
   "source": [
    "$ \\mathbf{S}_0 $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{S}_0^{new} = \\mathbb{E}[\\mathbf{z}_0 \\mathbf{z}_0^T] - \\mathbb{E}[\\mathbf{z}_0]\\mathbb{E}[\\mathbf{z}_0^T] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91b2e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_S0_new(E1, E4):\n",
    "    E1_0 = np.array([E1[0]])\n",
    "    E1_0_T = E1_0.transpose()\n",
    "    return np.subtract(\n",
    "        E4[0],\n",
    "        np.matmul(E1_0_T, E1_0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ee688b",
   "metadata": {},
   "source": [
    "$ \\mathbf{A} $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{A}^{new} = \\left( \\sum_{n=1}^{N-1} \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^T] \\right) \\left( \\sum_{n=0}^{N-2} \\mathbb{E}[\\mathbf{z}_{n} \\mathbf{z}_{n}^T] \\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5826d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_new(E2, E4):\n",
    "    return np.matmul(\n",
    "        np.sum(E2, axis=0),\n",
    "        np.linalg.inv(np.sum(E4[:N-1], axis=0))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163c8a5",
   "metadata": {},
   "source": [
    "$ \\mathbf{\\Gamma} $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{\\Gamma}^{new} = \\frac{1}{N - 1} \\sum_{n=1}^{N-1} \\left\\{ \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] - \\mathbf{A}^{new} \\mathbb{E}[\\mathbf{z}_{n-1} \\mathbf{z}_n^T] - \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_{n-1}^{T}](\\mathbf{A}^{new})^T + \\mathbf{A}^{new} \\mathbb{E}[\\mathbf{z}_{n-1}\\mathbf{z}_{n-1}^T](\\mathbf{A}^{new})^T \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b86809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Gamma_new(E2, E3, E4, A_new):\n",
    "    elems = np.empty((0,3,3))\n",
    "    for n in range(1, N):\n",
    "        elems_n = np.add(\n",
    "            np.subtract(\n",
    "                np.subtract(\n",
    "                    E4[n],\n",
    "                    np.matmul(\n",
    "                        A_new,\n",
    "                        E3[n-1]\n",
    "                    )\n",
    "                ),\n",
    "                np.matmul(\n",
    "                    E2[n-1],\n",
    "                    A_new.transpose()\n",
    "                )\n",
    "            ),\n",
    "            np.matmul(\n",
    "                np.matmul(\n",
    "                    A_new,\n",
    "                    E4[n-1]\n",
    "                ),\n",
    "                A_new.transpose()\n",
    "            )\n",
    "        )\n",
    "        elems = np.vstack((elems, [elems_n]))\n",
    "    return np.sum(elems, axis=0) / (N-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551c888b",
   "metadata": {},
   "source": [
    "$ \\mathbf{C} $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{C}^{new} = \\left( \\sum_{n=0}^{N-1} \\mathbf{x}_n \\mathbb{E}[\\mathbf{z}_n]^T \\right) \\left( \\sum_{n=0}^{N-1} \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] \\right)^{-1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0de357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_C_new_former(E1):\n",
    "    elems = np.empty((0,2,3))\n",
    "    for n in range(N):\n",
    "        x_n_T = np.array([X[n]]).transpose()\n",
    "        E1_n = np.array([E1[n]])\n",
    "        elems_n = np.matmul(x_n_T, E1_n)\n",
    "        elems = np.vstack((elems, [elems_n]))\n",
    "    return np.sum(elems, axis=0)\n",
    "\n",
    "def get_C_new(E1, E4):\n",
    "    return np.matmul(\n",
    "        get_C_new_former(E1),\n",
    "        np.linalg.inv(np.sum(E4, axis=0))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ad181",
   "metadata": {},
   "source": [
    "$ \\mathbf{\\Sigma} $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{\\Sigma}^{new} = \\frac{1}{N} \\sum_{n=0}^{N-1} \\left\\{ \\mathbf{x}_n \\mathbf{x}_n^T - \\mathbf{C}^{new} \\mathbb{E}[\\mathbf{z}_n] \\mathbf{x}_n^T - \\mathbf{x}_n \\mathbb{E}[\\mathbf{z}_n^T](\\mathbf{C}^{new})^T + \\mathbf{C}^{new} \\mathbb{E}[\\mathbf{z}_n \\mathbf{z}_n^T] (\\mathbf{C}^{new})^T \\right\\} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec94e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Sigma_new(E1, E4, C_new):\n",
    "    elems = np.empty((0,2,2))\n",
    "    for n in range(N):\n",
    "        x_n = np.array([X[n]])\n",
    "        x_n_T = x_n.transpose()\n",
    "        E1_n = np.array([E1[n]])\n",
    "        E1_n_T = E1_n.transpose()\n",
    "        elem_n = np.add(\n",
    "            np.subtract(\n",
    "                np.subtract(\n",
    "                    np.matmul(x_n_T, x_n),\n",
    "                    np.matmul(\n",
    "                        np.matmul(\n",
    "                            C_new,\n",
    "                            E1_n_T\n",
    "                        ),\n",
    "                        x_n\n",
    "                    )\n",
    "                ),\n",
    "                np.matmul(\n",
    "                    np.matmul(\n",
    "                        x_n_T,\n",
    "                        E1_n\n",
    "                    ),\n",
    "                    C_new.transpose()\n",
    "                )\n",
    "            ),\n",
    "            np.matmul(\n",
    "                np.matmul(\n",
    "                    C_new,\n",
    "                    E4[n]\n",
    "                ),\n",
    "                C_new.transpose()\n",
    "            )\n",
    "        )\n",
    "        elems = np.vstack((elems, [elem_n]))\n",
    "    return np.sum(elems, axis=0) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786d473",
   "metadata": {},
   "source": [
    "## 5. Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "566f0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 100 ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for loop in range(100):\n",
    "    print(\"Running iteration {} ...\".format(loop + 1), end=\"\\r\")\n",
    "    # Get mu and V\n",
    "    V = get_V()\n",
    "    mu = get_mu(V)\n",
    "    # Get mu_hat and V_hat\n",
    "    V_hat = get_V_hat(V)\n",
    "    mu_hat = get_mu_hat(mu, V)\n",
    "    # Get expectation values\n",
    "    E1 = get_E1(mu_hat)\n",
    "    E2, E3 = get_E2_E3(V, mu_hat, V_hat)\n",
    "    E4 = get_E4(mu_hat, V_hat)\n",
    "    # Get optimized new parameters\n",
    "    m0_new = get_m0_new(E1)\n",
    "    S0_new = get_S0_new(E1, E4)\n",
    "    A_new = get_A_new(E2, E4)\n",
    "    Gamma_new = get_Gamma_new(E2, E3, E4, A_new)\n",
    "    C_new = get_C_new(E1, E4)\n",
    "    Sigma_new = get_Sigma_new(E1, E4, C_new)\n",
    "    # Replace theta and repeat\n",
    "    theta_old.m0 = m0_new\n",
    "    theta_old.S0 = S0_new\n",
    "    theta_old.A = A_new\n",
    "    theta_old.Gamma = Gamma_new\n",
    "    theta_old.C = C_new\n",
    "    theta_old.Sigma = Sigma_new\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17595e06",
   "metadata": {},
   "source": [
    "Here is the estimated results for parameters.\n",
    "\n",
    "I note that the result won't be unique (depending on scaling, rotation, etc) and this result is then one of such locally maximized results.<br>\n",
    "For your reference, I show you the simulated points $ \\{ \\mathbf{x}_n \\} $ (first 5 points in sequence) compared with observations in below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e45c752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m0\n",
      "[ 2.57496192  6.3276932  11.234217  ]\n",
      "S0\n",
      "[[5.24798097e-05 2.79583705e-05 8.54545730e-05]\n",
      " [2.79583705e-05 2.07409258e-04 1.78504551e-04]\n",
      " [8.54545730e-05 1.78504551e-04 3.76386362e-04]]\n",
      "A\n",
      "[[ 0.78563464 -0.54698622  0.37223684]\n",
      " [ 0.89487513  0.87198508 -0.1177087 ]\n",
      " [ 0.10580215  0.43482559  0.71643318]]\n",
      "Gamma\n",
      "[[0.00832995 0.02118287 0.03410333]\n",
      " [0.02118287 0.06644179 0.10506952]\n",
      " [0.03410333 0.10506952 0.17151925]]\n",
      "C\n",
      "[[-15.90921342  -3.14027935   9.66084486]\n",
      " [ 19.55862629   7.56719346  -4.3960726 ]]\n",
      "Sigma\n",
      "[[0.96929266 0.04601912]\n",
      " [0.04601912 1.87507473]]\n"
     ]
    }
   ],
   "source": [
    "print(\"m0\")\n",
    "print(m0_new)\n",
    "print(\"S0\")\n",
    "print(S0_new)\n",
    "print(\"A\")\n",
    "print(A_new)\n",
    "print(\"Gamma\")\n",
    "print(Gamma_new)\n",
    "print(\"C\")\n",
    "print(C_new)\n",
    "print(\"Sigma\")\n",
    "print(Sigma_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7d9865e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Observation *****\n",
      "[[47.68902127 48.86610347]\n",
      " [43.94953287 54.53438865]\n",
      " [41.39095419 55.59938853]\n",
      " [45.94610067 55.73904685]\n",
      " [53.12714417 48.54030761]]\n",
      "***** Simulated Result *****\n",
      "[[47.49019418 47.29451052]\n",
      " [44.14951187 57.2202532 ]\n",
      " [42.05716531 56.68219724]\n",
      " [43.07225681 55.11272219]\n",
      " [52.83202734 51.55821952]]\n"
     ]
    }
   ],
   "source": [
    "# Show simulated results\n",
    "\n",
    "N_sim = 4\n",
    "\n",
    "Z_sim = np.array([m0_new])\n",
    "for n in range(N_sim):\n",
    "    z_prev = Z_sim[len(Z_sim) - 1]\n",
    "    mean = np.matmul(A_new, z_prev)\n",
    "    z_post = np.random.multivariate_normal(\n",
    "        mean=mean,\n",
    "        cov=Gamma_new,\n",
    "        size=1)\n",
    "    Z_sim = np.vstack((Z_sim, z_post))\n",
    "X_sim = np.empty((0,2))\n",
    "for z_n in Z_sim:\n",
    "    mean = np.matmul(C_new, z_n)\n",
    "    x_n = np.random.multivariate_normal(\n",
    "        mean=mean,\n",
    "        cov=Sigma_new,\n",
    "        size=1)\n",
    "    X_sim = np.vstack((X_sim, x_n))\n",
    "print(\"***** Observation *****\")\n",
    "print(X[:5])\n",
    "print(\"***** Simulated Result *****\")\n",
    "print(X_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4c5fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
