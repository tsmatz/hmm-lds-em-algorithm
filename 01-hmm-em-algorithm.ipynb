{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fda67945",
   "metadata": {},
   "source": [
    "# Hidden Markov Models (HMM) in Python\n",
    "\n",
    "HMM can handle a wide range of distributions, such as, discrete tables, Gaussians, and mixtures of Gaussians.\n",
    "\n",
    "In HMM, it assumes the latent (hidden) discrete multinomial variables $ \\{\\mathbf{z}_n\\} $, which generates the corresponding observation $ \\{\\mathbf{x}_n\\} $. (See below.)<br>\n",
    "The observers can see only $ \\{\\mathbf{x}_n\\} $, and the model will then be estimated using obervations, $ \\{\\mathbf{x}_n\\} $.\n",
    "\n",
    "![Hidden Markov Models](images/hmm.png?raw=true)\n",
    "\n",
    "In HMM, $ p(\\mathbf{z}_n|\\mathbf{z}_{n-1}) $ is called a **transition probability**, and $ p(\\mathbf{x}_n|\\mathbf{z}_n) $ is a **emission probability**.\n",
    "\n",
    "> In this notebook, I denote a scalar variable by normal letter (such as, $ x $), and denote a vector (incl. a matrix) by bold letter (such as, $ \\mathbf{x} $).\n",
    "\n",
    "*back to [Readme](https://github.com/tsmatz/hmm-lds-em-algorithm/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7561acd9",
   "metadata": {},
   "source": [
    "##  Sampling in Hidden Markov Models (Generate sample data)\n",
    "\n",
    "First of all, we'll generate sample data (observations) by using the distribution of Hidden Markov Models (HMM).\n",
    "\n",
    "As I mentioned above, the distribution of the latent (hidden) variables $ \\{\\mathbf{z}_n\\} $ is discrete, and it then corresponds to a table of transitions.\n",
    "\n",
    "For sampling, first I'll create a set of latent (hidden) variables, $ \\{\\mathbf{z}_n\\} $, in which it has 3 states (i.e, $ K=3 $) with the following transition probabilities $ p(\\mathbf{z}_n|\\mathbf{z}_{n-1}) $.\n",
    "\n",
    "![HMM Discrete Transition](images/transition.png?raw=true)\n",
    "\n",
    "$$ A = \\begin{bmatrix} 0.7 & 0.15 & 0.15 \\\\ 0.0 & 0.5 & 0.5 \\\\ 0.3 & 0.35 & 0.35 \\end{bmatrix} $$\n",
    "\n",
    "From now, I'll use the letter $ k \\in \\{0, 1, 2\\} $ for the corresponding 3 states, and I assume $ \\mathbf{z}_n = (z_{n,0}, z_{n,1}, z_{n,2}) $, in which $ z_{n,k^{\\prime}}=1 $ and $ z_{n,k \\neq k^{\\prime}}=0 $ in state $ k^{\\prime} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c961a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1000)  # To evaluate results\n",
    "\n",
    "N = 1000\n",
    "\n",
    "Z = np.array([0])\n",
    "for n in range(N):\n",
    "    prev_z = Z[len(Z) - 1]\n",
    "    if prev_z == 0:\n",
    "        post_z = np.random.choice(3, size=1, p=[0.7, 0.15, 0.15])\n",
    "    elif prev_z == 1:\n",
    "        post_z = np.random.choice(3, size=1, p=[0.0, 0.5, 0.5])\n",
    "    elif prev_z == 2:\n",
    "        post_z = np.random.choice(3, size=1, p=[0.3, 0.35, 0.35])\n",
    "    Z = np.append(Z, post_z)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751b087",
   "metadata": {},
   "source": [
    "Next I'll create the corresponding observation $ \\{\\mathbf{x}_n\\} $ for sampling.<br>\n",
    "Here I assume 2-dimensional **Gaussian distribution** $ \\mathcal{N}(\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k) $ for emission probabilities $ p(\\mathbf{x}_n|\\mathbf{z}_n) $, when $ \\mathbf{z}_n $ belongs to $ k $. ($ k=0,1,2 $)<br>\n",
    "In order to simplify, I also assume that parameters $ \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k $ are independent for different components $ k=0, 1, 2 $.\n",
    "\n",
    "In this example, I set $ \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k $ as follows.\n",
    "\n",
    "$$ \\mathbf{\\mu}_0=(16.0, 1.0), \\;\\; \\mathbf{\\Sigma}_0 = \\begin{bmatrix} 4.0 & 3.5 \\\\ 3.5 & 4.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\mu}_1=(1.0, 16.0), \\;\\; \\mathbf{\\Sigma}_1 = \\begin{bmatrix} 4.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\mu}_2=(-5.0, -5.0), \\;\\; \\mathbf{\\Sigma}_2 = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 4.0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d18dbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16.10996367, -0.05478763],\n",
       "       [18.15392063,  3.77525205],\n",
       "       [16.73825958,  0.59324625],\n",
       "       ...,\n",
       "       [14.2188323 , -1.0984775 ],\n",
       "       [18.41063372,  5.28130838],\n",
       "       [-3.64054111, -4.00216984]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.empty((0,2))\n",
    "for z_n in Z:\n",
    "    if z_n == 0:\n",
    "        x_n = np.random.multivariate_normal(\n",
    "            mean=[16.0, 1.0],\n",
    "            cov=[[4.0,3.5],[3.5,4.0]],\n",
    "            size=1)\n",
    "    elif z_n == 1:\n",
    "        x_n = np.random.multivariate_normal(\n",
    "            mean=[1.0, 16.0],\n",
    "            cov=[[4.0,0.0],[0.0,1.0]],\n",
    "            size=1)\n",
    "    elif z_n ==2:\n",
    "        x_n = np.random.multivariate_normal(\n",
    "            mean=[-5.0, -5.0],\n",
    "            cov=[[1.0,0.0],[0.0,4.0]],\n",
    "            size=1)\n",
    "    X = np.vstack((X, x_n))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb896c56",
   "metadata": {},
   "source": [
    "## EM algorithm in Hidden Markov Models (HMM)\n",
    "\n",
    "Now, using the given observation $ \\{ \\mathbf{x}_n \\} $, let's try to estimate the optimimal parameters in HMM.\n",
    "\n",
    "When I denote unknown parameters by $ \\mathbf{\\theta} $, our goal is to get the optimal parameters $ \\mathbf{\\theta} $ to maximize the following (1).\n",
    "\n",
    "$$ p(\\mathbf{X}|\\mathbf{\\theta}) = \\sum_{\\mathbf{Z}} p(\\mathbf{X},\\mathbf{Z}|\\mathbf{\\theta}) \\;\\;\\;\\;(1) $$\n",
    "\n",
    "where $ \\mathbf{Z} = \\{\\mathbf{z}_n\\} $ and $ \\mathbf{X} = \\{\\mathbf{x}_n\\} $\n",
    "\n",
    "In this example, I use the following parameters as $ \\mathbf{\\theta} = \\{ \\mathbf{\\pi}, \\mathbf{A}, \\mathbf{\\mu}, \\mathbf{\\Sigma} \\} $.\n",
    "\n",
    "- $ \\pi_k (k \\in \\{0, 1, 2\\}) $ : The possibility (scalar) for component $ k $ in initial latent node $ \\mathbf{z}_0 $. ($ \\Sigma_k \\pi_k = 1 $)\n",
    "- $ A_{j,k} \\; (j, k \\in \\{0, 1, 2\\}) $ : The transition probability (scalar) for the latent variable $ \\mathbf{z}_{n-1} $ to $ \\mathbf{z}_n $, in which $ \\mathbf{z}_{n-1} $ belongs to $ j $ and $ \\mathbf{z}_n $ belongs to $ k $. ($ \\Sigma_k A_{j,k} = 1 $)\n",
    "- $ \\mathbf{\\mu}_k $ : The mean (2-dimensional vector) for Gaussian distribution in emission probabilities $ p(\\mathbf{x}_n|\\mathbf{z}_n) $ when the latent variable $ \\mathbf{z}_n $ belongs to $ k $.\n",
    "- $ \\mathbf{\\Sigma}_k $ : The covariance matrix ($ 2 \\times 2 $ matrix) for Gaussian distribution in emission probabilities $ p(\\mathbf{x}_n|\\mathbf{z}_n) $ when the latent variable $ \\mathbf{z}_n $ belongs to $ k $.\n",
    "\n",
    "In (1), the number of parameters will rapidly increase, when the number of states $ K $ increases (in this example, $ K = 3 $). Furthermore it has summation (not multiplication) in distribution (1), and the log likelihood will then lead to complex expression in maximum likelihood estimation (MLE).<br>\n",
    "Therefore, it will be difficult to directly apply maximum likelihood estimation (MLE) for the expression (1).\n",
    "\n",
    "> Note : Please see [here](https://tsmatz.wordpress.com/2017/08/30/glm-regression-logistic-poisson-gaussian-gamma-tutorial-with-r/) for the idea of maximum likelihood estimation (MLE).\n",
    "\n",
    "In practice, the expectationâ€“maximization algorithm (shortly, **EM algorithm**) can often be applied to solve parameters in HMM.<br>\n",
    "In this example, I'll also apply EM algorithm, instead of MLE.\n",
    "\n",
    "In EM algorithm for HMM, we start with initial parameters $ \\mathbf{\\theta}^{old} $, and optimize (find) new $ \\mathbf{\\theta} $ to maximize the following expression (2).<br>\n",
    "By repeating this operation, we can expect to reach to the likelihood parameters $ \\hat{\\mathbf{\\theta}} $.\n",
    "\n",
    "$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{old}) = \\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X}, \\mathbf{\\theta}^{old}) \\ln p(\\mathbf{X}, \\mathbf{Z}|\\mathbf{\\theta}) \\;\\;\\;\\;(2) $$\n",
    "\n",
    "> Note : For the essential idea of EM algorithm, see Chapter 9 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft)\n",
    "\n",
    "Now I denote the discrete probability $ p(\\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $ by $ \\gamma(z_{n,k}) \\; (k=0,1,2) $, in which $ \\gamma(z_{n,k}) $ represents the probability of $ \\mathbf{z}_n $ for belonging to $ k $.<br>\n",
    "I also denote the discrete probability $ p(\\mathbf{z}_{n-1}, \\mathbf{z}_n | \\mathbf{X},\\mathbf{\\theta}^{old}) $ by $ \\xi(z_{n-1,j}, z_{n,k}) \\; (j,k=0,1,2) $, in which $ \\xi(z_{n-1,j}, z_{n,k}) $ represents the joint probability that $ \\mathbf{z}_{n-1} $ belongs to $ j $ and $ \\mathbf{z}_n $ belongs to $ k $. \n",
    "\n",
    "In Gaussian HMM (in above model), the equation (2) is written as follows, using $ \\gamma() $ and $ \\xi() $.\n",
    "\n",
    "$$ Q(\\mathbf{\\theta}, \\mathbf{\\theta}^{old}) = \\sum_{k=0}^{K-1} \\gamma(z_{0,k}) \\ln{\\pi_k} + \\sum_{n=1}^{N-1} \\sum_{j=0}^{K-1} \\sum_{k=0}^{K-1} \\xi(z_{n-1,j},z_{n,k}) \\ln{A_{j,k}} + \\sum_{n=0}^{N-1} \\sum_{k=0}^{K-1} \\gamma(z_{n,k}) \\ln{p(\\mathbf{x}_n|\\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k)} \\;\\;\\;\\;(3)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ \\gamma(\\mathbf{z}_n) = p(\\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $$\n",
    "\n",
    "$$ \\xi(\\mathbf{z}_{n-1}, \\mathbf{z}_n) = p(\\mathbf{z}_{n-1}, \\mathbf{z}_n|\\mathbf{X},\\mathbf{\\theta}^{old}) $$\n",
    "\n",
    "It's known that $ \\gamma() $ and $ \\xi() $ can be given by the following $ \\alpha() $ and $ \\beta() $, which are determined recursively. (i.e, We can first determine all $ \\alpha() $ and $ \\beta() $ recursively, and then we can obtain $ \\gamma() $ and $ \\xi() $ with known $ \\alpha(), \\beta() $.)\n",
    "\n",
    "$$ \\gamma(z_{n,k}) = \\frac{\\alpha(z_{n,k})\\beta(z_{n,k})}{\\sum_{k=0}^{K-1} \\alpha(z_{n,k})\\beta(z_{n,k})} $$\n",
    "\n",
    "$$ \\xi(z_{n-1,j},z_{n,k}) = \\frac{\\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})}{\\sum_{j=0}^{K-1} \\sum_{k=0}^{K-1} \\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})} $$\n",
    "\n",
    "where all $ \\alpha() $ and $ \\beta() $ are recursively given by\n",
    "\n",
    "$$ \\alpha(z_{n,k}) = p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) \\sum_{j=0}^{K-1} A_{jk}^{old} \\alpha(z_{n-1,j}) $$\n",
    "\n",
    "$$ \\beta(z_{n-1,k}) = \\sum_{j=0}^{K-1} A^{old}_{k,j} p(\\mathbf{x}_{n}|\\mathbf{\\mu}_j^{old}, \\mathbf{\\Sigma}_j^{old}) \\beta(z_{n,j}) $$\n",
    "\n",
    "Now we need the starting condition for recursion, $ \\alpha() $ and $ \\beta() $, and these are given as follows.\n",
    "\n",
    "$$ \\alpha(z_{0,k}) = \\pi_k^{old} p(\\mathbf{x}_0|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) $$\n",
    "\n",
    "$$ \\beta(z_{N-1,k}) = 1 $$\n",
    "\n",
    "> Note : Here I have showed these properties in Gaussian HMM without any proofs, but you can refer Chapter 13 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" (Christopher M. Bishop, Microsoft) for details. (In this notebook, I'm using the same notation in this book.)\n",
    "\n",
    "Once you have got $ \\gamma() $ and $ \\xi() $, you can get the optimal $ \\mathbf{\\theta} = \\{ \\mathbf{\\pi}, \\mathbf{A}, \\mathbf{\\mu}, \\mathbf{\\Sigma} \\} $ to maximize (3) as follows by applying Lagrange multipliers.\n",
    "\n",
    "$$ \\pi_k = \\frac{\\gamma(z_{0,k})}{\\sum_{j=0}^{K-1} \\gamma(z_{0,j})} $$\n",
    "\n",
    "$$ A_{j,k} = \\frac{\\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,k})}{\\sum_{l=0}^{K-1} \\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,l})} $$\n",
    "\n",
    "$$ \\mathbf{\\mu}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) \\mathbf{x}_n}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$\n",
    "\n",
    "$$ \\mathbf{\\Sigma}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) (\\mathbf{x}_n-\\mathbf{\\mu}_k) (\\mathbf{x}_n-\\mathbf{\\mu}_k)^T}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$\n",
    "\n",
    "You repeat this process by replacing $ \\mathbf{\\theta}^{old} $ with this new $ \\mathbf{\\theta} $, and you will eventually get the optimal results $ \\hat{\\mathbf{\\theta}} $ to maximize (1).\n",
    "\n",
    "In practice, $ \\alpha() $ and $ \\beta() $ will quickly go to zero (because it's recursively multiplied by $ p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) $ and $ A_{j,k}^{old} $) and it will then exceed the dynamic range of precision in computation, when $ N $ is large.<br>\n",
    "For this reason, the coefficients, called **scaling factors**, will be introduced to normalize $ \\alpha() $ and $ \\beta() $ in each step $ n $. (See the following source code.) The scaling factors will be canceled in EM algorithms, however, when you monitor the value of likelihood functions, you'll need to record scaling factors and apply these facotrs. (See Chapter 13 in \"[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf?ranMID=24542&ranEAID=TnL5HPStwNw&ranSiteID=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&epi=TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ&irgwc=1&OCID=AID2200057_aff_7593_1243925&tduid=%28ir__vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300%29%287593%29%281243925%29%28TnL5HPStwNw-g4zE85KQgCXaCQfYBhtuFQ%29%28%29&irclickid=_vhvv9m6caokf6nb62oprh029if2xo0rux3ga300300)\" for details.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba1a21e",
   "metadata": {},
   "source": [
    "## Apply algorithm in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f60f500",
   "metadata": {},
   "source": [
    "## 0. Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4baaa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3be6ac",
   "metadata": {},
   "source": [
    "## 1. Initialize parameters\n",
    "\n",
    "First, initialize $ \\mathbf{\\theta} = \\{ \\pi_k, \\mathbf{A}_{j,k}, \\mathbf{\\mu}_k, \\mathbf{\\Sigma}_k \\} $ as follows.\n",
    "\n",
    "- $ \\pi_0 = 0.3, \\pi_1 = 0.3, \\pi_2 = 0.4 $\n",
    "- $ A_{i,j} = 0.4 $ if $ i=j $, and $ A_{i,j} = 0.3 $ otherwise\n",
    "- $ \\mathbf{\\mu}_k = (1.0, 1.0) \\;\\;\\; (k = 0,1,2) $\n",
    "- $ \\mathbf{\\Sigma}_k = \\begin{bmatrix} 1.0 & 0.5 \\\\ 0.5 & 1.0 \\end{bmatrix} \\;\\;\\; (k = 0,1,2) $\n",
    "\n",
    "In this example, I set the fixed values. However, in practice, K-means will be used to determine initial $ \\mathbf{\\mu}_k $ and $ \\mathbf{\\Sigma}_k $, in order to speed up optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b114a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "theta_old = {\n",
    "    \"pi\":[0.3, 0.3, 0.4],\n",
    "    \"A\":[[0.4,0.3,0.3],[0.3,0.4,0.3],[0.3,0.3,0.4]],\n",
    "    \"mu\":[[1.0,1.0],[1.0,1.0],[1.0,1.0]],\n",
    "    \"Sigma\":[\n",
    "        [[1.0,0.5],[0.5,1.0]],\n",
    "        [[1.0,0.5],[0.5,1.0]],\n",
    "        [[1.0,0.5],[0.5,1.0]]\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb646728",
   "metadata": {},
   "source": [
    "## 2. Get $ \\alpha() $ and $ \\beta() $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a4fc8",
   "metadata": {},
   "source": [
    "Now I set the starting condition, $ \\alpha(z_{0,k}) $. :\n",
    "\n",
    "$$ \\alpha(z_{0,k}) = \\pi_k^{old} p(\\mathbf{x}_0|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) $$\n",
    "\n",
    "And we can recursively obtain all $ \\alpha(z_{n,k}) $ as follows.\n",
    "\n",
    "$$ \\alpha(z_{n,k}) = p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old}) \\sum_{j=0}^{K-1} A_{jk}^{old} \\alpha(z_{n-1,j}) $$\n",
    "\n",
    "As I mentioned above, I also introduce a scaling factor in each steps to prevent the overflow of dynamic range. (Here I don't record these scaling factors.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "179fc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "def get_alpha():\n",
    "    alpha = np.empty((0,3))\n",
    "\n",
    "    # Get initial alpha_0\n",
    "    alpha_0 = np.array([])\n",
    "    for k in range(3):\n",
    "        p_dist = multivariate_normal(\n",
    "            mean=theta_old[\"mu\"][k],\n",
    "            cov=theta_old[\"Sigma\"][k])\n",
    "        alpha_0 = np.append(alpha_0, theta_old[\"pi\"][k] * p_dist.pdf(X[0]))\n",
    "    alpha_0 = alpha_0 / alpha_0.sum()  # apply scaling\n",
    "    alpha = np.vstack((alpha, alpha_0))\n",
    "\n",
    "    # Get all elements recursively\n",
    "    for n in range(1, N):\n",
    "        alpha_n = np.array([])\n",
    "        for k in range(3):\n",
    "            p_dist = multivariate_normal(\n",
    "                mean=theta_old[\"mu\"][k],\n",
    "                cov=theta_old[\"Sigma\"][k])\n",
    "            alpha_n = np.append(\n",
    "                alpha_n,\n",
    "                p_dist.pdf(X[n]) * sum((theta_old[\"A\"][j][k] * alpha[n-1][j]) for j in range(3)))\n",
    "        alpha_n = alpha_n / alpha_n.sum()  # apply scaling\n",
    "        alpha = np.vstack((alpha, alpha_n))\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a216fa5",
   "metadata": {},
   "source": [
    "I also set the starting condition, $ \\beta(z_{N-1,k}) $. :\n",
    "\n",
    "$$ \\beta(z_{N-1,k}) = 1 $$\n",
    "\n",
    "And we can recursively obtain all $ \\beta(z_{n,k}) $ as follows.\n",
    "\n",
    "$$ \\beta(z_{n-1,k}) = \\sum_{j=0}^{K-1} A^{old}_{k,j} p(\\mathbf{x}_{n}|\\mathbf{\\mu}_j^{old}, \\mathbf{\\Sigma}_j^{old}) \\beta(z_{n,j}) $$\n",
    "\n",
    "As I mentioned above, I also introduce a scaling factor in each steps to prevent the overflow of dynamic range. In practice, the scaling factors can be shared between $ \\alpha() $ and $ \\beta() $ (and you can then use these shared values for getting values of likelihood function), but in this example, I simply normalize values in each steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e94369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_beta():\n",
    "    beta_rev = np.empty((0,3))\n",
    "\n",
    "    # Get initial beta_{N-1}\n",
    "    beta_last = np.array([1.0, 1.0, 1.0])\n",
    "    beta_last = beta_last / beta_last.sum()  # apply scaling\n",
    "    beta_rev = np.vstack((beta_rev, beta_last))\n",
    "\n",
    "    # Get all elements recursively\n",
    "    for n in range(1, N):\n",
    "        beta_rev_n = np.array([])\n",
    "        for k in range(3):\n",
    "            beta_rev_n_k = 0\n",
    "            for j in range(3):\n",
    "                p_dist = multivariate_normal(\n",
    "                    mean=theta_old[\"mu\"][j],\n",
    "                    cov=theta_old[\"Sigma\"][j])\n",
    "                beta_rev_n_k = theta_old[\"A\"][k][j] * p_dist.pdf(X[n-1]) * beta_rev[n-1][j] + beta_rev_n_k\n",
    "            beta_rev_n = np.append(beta_rev_n, beta_rev_n_k)\n",
    "        beta_rev_n = beta_rev_n / beta_rev_n.sum()  # apply scaling\n",
    "        beta_rev = np.vstack((beta_rev, beta_rev_n))\n",
    "\n",
    "    # Reverse results\n",
    "    beta = np.flip(beta_rev, axis=0)\n",
    "    \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486bdfc",
   "metadata": {},
   "source": [
    "## 3. Get $ \\gamma() $ and $ \\xi() $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee549e91",
   "metadata": {},
   "source": [
    "Now we obtain $ \\gamma() $ and $ \\xi() $ with previous $ \\alpha() $ and $ \\beta() $.<br>\n",
    "First we get $ \\gamma() $ as follows. (I note that the value is not normalized.)\n",
    "\n",
    "$$ \\gamma(z_{n,k}) = \\frac{\\alpha(z_{n,k})\\beta(z_{n,k})}{\\sum_{k=0}^{K-1} \\alpha(z_{n,k})\\beta(z_{n,k})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4282314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gamma(alpha, beta):\n",
    "    gamma = np.empty((0,3))\n",
    "\n",
    "    for n in range(N):\n",
    "        gamma_n = np.array([])\n",
    "        for k in range(3):\n",
    "            gamma_n = np.append(gamma_n, alpha[n][k] * beta[n][k])\n",
    "        gamma_n = gamma_n / gamma_n.sum()\n",
    "        gamma = np.vstack((gamma, gamma_n))\n",
    "\n",
    "    return gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0da770",
   "metadata": {},
   "source": [
    "Next we also get $ \\xi() $ as follows. (I note that the value is not normalized.)\n",
    "\n",
    "$$ \\xi(z_{n-1,j},z_{n,k}) = \\frac{\\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})}{\\sum_{j=0}^{K-1} \\sum_{k=0}^{K-1} \\alpha(z_{n-1,j})p(\\mathbf{x}_n|\\mathbf{\\mu}_k^{old}, \\mathbf{\\Sigma}_k^{old})A_{j,k}^{old}\\beta(z_{n,k})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec9dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xi(alpha, beta):\n",
    "    xi = np.empty((0,3,3))\n",
    "\n",
    "    for n in range(1, N):\n",
    "        xi_n = np.zeros((3,3), dtype=np.float64)\n",
    "        for j in range(3):\n",
    "            for k in range(3):\n",
    "                p_dist = multivariate_normal(\n",
    "                    mean=theta_old[\"mu\"][k],\n",
    "                    cov=theta_old[\"Sigma\"][k])\n",
    "                xi_n[j][k] = alpha[n-1][j] * p_dist.pdf(X[n]) * theta_old[\"A\"][j][k] * beta[n][k]\n",
    "        xi_n = xi_n / xi_n.sum()\n",
    "        xi = np.vstack((xi, [xi_n]))\n",
    "\n",
    "    return xi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27e5dc",
   "metadata": {},
   "source": [
    "## 4. Get new (optimal) parameters $ \\mathbf{\\theta} $\n",
    "\n",
    "Finally, get new $ \\mathbf{\\theta} = \\{ \\pi_k, A, \\mathbf{\\mu}, \\mathbf{\\Sigma} \\} $ using previous $ \\gamma() $ and $ \\xi() $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60333624",
   "metadata": {},
   "source": [
    "$ \\pi_k \\; (k=0,1,2) $ is given as follows.\n",
    "\n",
    "$$ \\pi_k = \\frac{\\gamma(z_{0,k})}{\\sum_{j=0}^{K-1} \\gamma(z_{0,j})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0b47105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pi_new(gamma):\n",
    "    pi_new = np.array([])\n",
    "\n",
    "    denom = sum(gamma[0][j] for j in range(3))\n",
    "    for k in range(3):\n",
    "        pi_new = np.append(pi_new, gamma[0][k] / denom)\n",
    "\n",
    "    return pi_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae3cdf",
   "metadata": {},
   "source": [
    "$ A_{j,k} \\; (j,k=0,1,2) $ is given as follows.\n",
    "\n",
    "$$ A_{j,k} = \\frac{\\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,k})}{\\sum_{l=0}^{K-1} \\sum_{n=1}^{N-1} \\xi(z_{n-1,j},z_{n,l})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a686f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_A_new(xi):\n",
    "    A_new = np.zeros((3,3), dtype=np.float64)\n",
    "\n",
    "    for j in range(3):\n",
    "        for k in range(3):\n",
    "            denom = 0\n",
    "            for l in range(3):\n",
    "                for n in range(1, N):\n",
    "                    denom = denom + xi[n-1][j][l]\n",
    "            A_new[j][k] = sum(xi[n-1][j][k] for n in range(1, N)) / denom\n",
    "\n",
    "    return A_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6dae4a",
   "metadata": {},
   "source": [
    "$ \\mathbf{\\mu}_{k} \\; (k=0,1,2) $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{\\mu}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) \\mathbf{x}_n}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98940314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_new(gamma):\n",
    "    mu_new = np.zeros((3,2), dtype=np.float64)\n",
    "\n",
    "    for k in range(3):\n",
    "        denom = sum(gamma[n][k] for n in range(N))\n",
    "        numer_x = sum(gamma[n][k] * X[n][0] for n in range(N))\n",
    "        mu_new[k][0] = numer_x / denom\n",
    "        numer_y = sum(gamma[n][k] * X[n][1] for n in range(N))\n",
    "        mu_new[k][1] = numer_y / denom\n",
    "\n",
    "    return mu_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5659c",
   "metadata": {},
   "source": [
    "$ \\mathbf{\\Sigma}_{k} \\; (k=0,1,2) $ is given as follows.\n",
    "\n",
    "$$ \\mathbf{\\Sigma}_k = \\frac{\\sum_{n=0}^{N-1} \\gamma(z_{n,k}) (\\mathbf{x}_n-\\mathbf{\\mu}_k) (\\mathbf{x}_n-\\mathbf{\\mu}_k)^T}{\\sum_{n=0}^{N-1} \\gamma(z_{n,k})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52d9e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Sigma_new(gamma, mu_new):\n",
    "    Sigma_new = np.empty((0,2,2))\n",
    "\n",
    "    for k in range(3):\n",
    "        denom = sum(gamma[n][k] for n in range(N))\n",
    "        numer = np.zeros((2, 2), dtype=np.float64)\n",
    "        for n in range(N):\n",
    "            sub = np.subtract(X[n], mu_new[k])\n",
    "            sub = np.array([sub])\n",
    "            sub_t = sub.transpose()\n",
    "            numer = numer + gamma[n][k] * np.matmul(sub_t, sub)\n",
    "        Sigma_new = np.vstack((Sigma_new, [numer / denom]))\n",
    "\n",
    "    return Sigma_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786d473",
   "metadata": {},
   "source": [
    "## 5. Run algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "566f0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running iteration 100 ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for loop in range(100):\n",
    "    print(\"Running iteration {} ...\".format(loop + 1), end=\"\\r\")\n",
    "    # Get alpha and beta\n",
    "    alpha = get_alpha()\n",
    "    beta = get_beta()\n",
    "    # Get gamma and xi\n",
    "    gamma = get_gamma(alpha, beta)\n",
    "    xi = get_xi(alpha, beta)\n",
    "    # Get optimized new parameters\n",
    "    pi_new = get_pi_new(gamma)\n",
    "    A_new = get_A_new(xi)\n",
    "    mu_new = get_mu_new(gamma)\n",
    "    Sigma_new = get_Sigma_new(gamma, mu_new)\n",
    "    # Replace theta and repeat\n",
    "    theta_old[\"pi\"] = pi_new\n",
    "    theta_old[\"A\"] = A_new\n",
    "    theta_old[\"mu\"] = mu_new\n",
    "    theta_old[\"Sigma\"] = Sigma_new\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09c21f",
   "metadata": {},
   "source": [
    "Here is the estimated results for parameters.<br>\n",
    "I note that the class ($ k $) in results will be transposed and thus I have rearranged its order in the following description.\n",
    "\n",
    "$$ A^{pred} = \\begin{bmatrix} 0.71929825 & 0.06427734 & 0.21642441 \\\\ 0.0 & 0.23535639 & 0.76464361 \\\\ 0.32796274 & 0.20301841 & 0.46901885 \\end{bmatrix} \\;\\; A^{label} = \\begin{bmatrix} 0.7 & 0.15 & 0.15 \\\\ 0.0 & 0.5 & 0.5 \\\\ 0.3 & 0.35 & 0.35 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\mu}_0^{pred}=(16.07330508, 0.9698968) \\;\\; \\mathbf{\\mu}_0^{label}=(16.0, 1.0), \\\\ \\mathbf{\\Sigma}_0^{pred} = \\begin{bmatrix} 3.99411283 & 3.47671034 \\\\ 3.47671034 & 4.06141304 \\end{bmatrix} \\;\\; \\mathbf{\\Sigma}_0^{label} = \\begin{bmatrix} 4.0 & 3.5 \\\\ 3.5 & 4.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\mu}_1^{pred}=(1.10889991, 16.13350166) \\;\\; \\mathbf{\\mu}_1^{label}=(1.0, 16.0), \\\\ \\mathbf{\\Sigma}_1^{pred} = \\begin{bmatrix} 4.5466816 & 0.06402755 \\\\ 0.06402755 & 0.93057687 \\end{bmatrix} \\;\\; \\mathbf{\\Sigma}_1^{label} = \\begin{bmatrix} 4.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix} $$\n",
    "\n",
    "$$ \\mathbf{\\mu}_2^{pred}=(-3.16503151, 1.4672828) \\;\\; \\mathbf{\\mu}_2^{label}=(-5.0, -5.0), \\\\ \\mathbf{\\Sigma}_2^{pred} = \\begin{bmatrix} 9.10191777 & 26.13881586 \\\\ 26.13881586 & 97.13257096 \\end{bmatrix} \\;\\; \\mathbf{\\Sigma}_2^{label} = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 4.0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e725701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "[[0.46901885 0.20301841 0.32796274]\n",
      " [0.76464361 0.23535639 0.        ]\n",
      " [0.21642441 0.06427734 0.71929825]]\n",
      "Mu\n",
      "[[-3.16503151  1.4672828 ]\n",
      " [ 1.10889991 16.13350166]\n",
      " [16.07330508  0.9698968 ]]\n",
      "Sigma\n",
      "[[[ 9.10191777 26.13881586]\n",
      "  [26.13881586 97.13257096]]\n",
      "\n",
      " [[ 4.5466816   0.06402755]\n",
      "  [ 0.06402755  0.93057687]]\n",
      "\n",
      " [[ 3.99411283  3.47671034]\n",
      "  [ 3.47671034  4.06141304]]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(\"A\")\n",
    "print(A_new)\n",
    "print(\"Mu\")\n",
    "print(mu_new)\n",
    "print(\"Sigma\")\n",
    "print(Sigma_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d9865e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
